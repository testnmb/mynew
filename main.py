import requests
import re
import os

# å…¨å±€æ’é™¤å…³é”®è¯å®šä¹‰
EXCLUDE_KEYWORDS = ["æˆäºº", "æ¿€æƒ…", "æƒ…è‰²", "æ¶©æƒ…", "18ç¦", "R18"]

class TVSourceProcessor:
    def __init__(self):
        self.all_lines = []
    
    def fetch_url_content(self, url: str):
        try:
            print(f"ğŸ”— è·å–URL: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # å°è¯•å¤šç§ç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'big5']
            content = None
            for encoding in encodings:
                try:
                    content = response.content.decode(encoding)
                    break
                except:
                    continue
            
            if content is None:
                content = response.content.decode('utf-8', errors='ignore')
            
            lines = [line.strip() for line in content.splitlines() if line.strip()]
            print(f"  æˆåŠŸè·å– {len(lines)} è¡Œ")
            return lines
            
        except Exception as e:
            print(f"  âŒ è·å–å¤±è´¥: {e}")
            return []
    
    def fetch_multiple_urls(self, urls: list):
        self.all_lines = []
        for url in urls:
            lines = self.fetch_url_content(url)
            if lines:
                self.all_lines.extend(lines)
        
        if self.all_lines:
            print(f"\nğŸ“Š æ€»è®¡è·å– {len(self.all_lines)} è¡Œå†…å®¹")
            # æ˜¾ç¤ºå‰å‡ è¡Œç¤ºä¾‹
            print("å‰5è¡Œç¤ºä¾‹:")
            for i, line in enumerate(self.all_lines[:5]):
                print(f"  {i+1}. {line[:80]}..." if len(line) > 80 else f"  {i+1}. {line}")
        else:
            print("\nâš ï¸  æœªè·å–åˆ°ä»»ä½•å†…å®¹")
    
    def remove_excluded_sections(self):
        if not self.all_lines:
            return []
        
        result = []
        in_excluded_section = False
        excluded_count = 0
        
        print(f"\nğŸ” å¼€å§‹æ’é™¤å¤„ç†...")
        for line in self.all_lines:
            if "#genre#" in line:
                if any(keyword in line for keyword in EXCLUDE_KEYWORDS):
                    if not in_excluded_section:  # æ–°å¼€å§‹ä¸€ä¸ªæ’é™¤åŒºåŸŸ
                        excluded_count += 1
                        print(f"  ğŸš« æ’é™¤åŒºåŸŸ {excluded_count}: {line[:60]}...")
                    in_excluded_section = True
                else:
                    in_excluded_section = False
                    result.append(line)
            elif not in_excluded_section:
                result.append(line)
        
        print(f"  å…±æ’é™¤ {excluded_count} ä¸ªåŒºåŸŸ")
        print(f"  å‰©ä½™ {len(result)} è¡Œ")
        return result
    
    def remove_genre_lines_and_deduplicate(self, lines: list):
        result = []
        seen_urls = set()
        genre_count = 0
        duplicate_count = 0
        
        print(f"\nâœ¨ å¼€å§‹å»é‡å¤„ç†...")
        for line in lines:
            # è·³è¿‡æ‰€æœ‰åŒ…å«#genre#çš„è¡Œ
            if "#genre#" in line:
                genre_count += 1
                continue
            
            # æå–URLè¿›è¡Œå»é‡
            url_match = re.search(r'(https?://[^\s,]+)', line)
            if url_match:
                url = url_match.group(1)
                if url not in seen_urls:
                    seen_urls.add(url)
                    result.append(line)
                else:
                    duplicate_count += 1
            else:
                # éURLè¡Œç›´æ¥æ·»åŠ 
                result.append(line)
        
        print(f"  åˆ é™¤ {genre_count} ä¸ª#genre#è¡Œ")
        print(f"  ç§»é™¤ {duplicate_count} ä¸ªé‡å¤URL")
        print(f"  å‰©ä½™ {len(result)} è¡Œ")
        return result
    
    def save_to_file(self, lines: list, filename: str = "my1.txt", first_line: str = "smt,#genre#"):
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
            
            # æ„å»ºå†…å®¹
            content = []
            if first_line:
                content.append(first_line)
            content.extend(lines)
            
            # å†™å…¥æ–‡ä»¶
            with open(filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))
            
            print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶: {filename}")
            print(f"  è¡Œæ•°: {len(content)}")
            print(f"  å¤§å°: {os.path.getsize(filename)} å­—èŠ‚")
            
            # æ˜¾ç¤ºä¿å­˜çš„å†…å®¹é¢„è§ˆ
            print("  å‰10è¡Œé¢„è§ˆ:")
            for i, line in enumerate(content[:10]):
                print(f"    {i+1}. {line[:80]}..." if len(line) > 80 else f"    {i+1}. {line}")
            
        except Exception as e:
            print(f"\nâŒ ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def process(self, urls: list, output_file: str = "my1.txt", first_line: str = "smt,#genre#"):
        print("=" * 60)
        print("ğŸ“º TVç›´æ’­æºå¤„ç†å·¥å…·")
        print("=" * 60)
        print(f"æ’é™¤å…³é”®è¯: {', '.join(EXCLUDE_KEYWORDS)}")
        print(f"å¤„ç†URLæ•°: {len(urls)}")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        print("=" * 60)
        
        # 1. è·å–URLå†…å®¹
        self.fetch_multiple_urls(urls)
        if not self.all_lines:
            print("\nâŒ é”™è¯¯: æœªè·å–åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®")
            return
        
        # 2. æ’é™¤å¤„ç†
        filtered_lines = self.remove_excluded_sections()
        if not filtered_lines:
            print("\nâš ï¸ è­¦å‘Š: æ’é™¤åæ— å‰©ä½™å†…å®¹")
            # å³ä½¿æ— å†…å®¹ä¹Ÿä¿å­˜ç©ºæ–‡ä»¶
            self.save_to_file([], output_file, first_line)
            return
        
        # 3. å»é‡å¤„ç†
        final_lines = self.remove_genre_lines_and_deduplicate(filtered_lines)
        
        # 4. ä¿å­˜ç»“æœ
        self.save_to_file(final_lines, output_file, first_line)
        
        print("=" * 60)
        print("âœ… å¤„ç†å®Œæˆ!")
        print("=" * 60)


def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    # è¦å¤„ç†çš„URLåˆ—è¡¨
    urls = [
        "https://raw.githubusercontent.com/FGBLH/FG/refs/heads/main/æ–¯ç‘ªç‰¹ç›´æ’­æº1",
        #"https://raw.githubusercontent.com/FGBLH/FG/refs/heads/main/æ–¯ç‘ªç‰¹ç›´æ’­æº2",
    ]
   
    # é…ç½®å‚æ•°
    output_file = "my1.txt"
    first_line = "smt,#genre#"
    
    # æ‰§è¡Œå¤„ç†
    processor = TVSourceProcessor()
    processor.process(urls, output_file, first_line)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(output_file):
        print(f"\nğŸ“ æ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}")
    else:
        print(f"\nâŒ æ–‡ä»¶æœªåˆ›å»º: {output_file}")


if __name__ == "__main__":
    main()
